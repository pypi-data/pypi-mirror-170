{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to set up a manual optimisation\n",
    "\n",
    "If you do not wish to use the Optimisation.run function to start an optimisation, you can manually set it up yourself. Examples of when you may want to do this include if you want to utilise NEMO as the optimisation algorithm within your own complex scripts such as for self-optimisating experimental platforms.\n",
    "\n",
    "This tutorial will take you through the steps needed to set up a manual optimisation:\n",
    "1. Define the variables, objectives, sampler, and acquisition function\n",
    "2. Generate initial samples to use as the X array for the training set\n",
    "3. After the user has collected the corresponding Y values for the training set, obtain some suggested candidates for an optimisation iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the variable, objectives, sampler, acquisition function, and the optimisation classes\n",
    "from nemo_bo.opt.variables import ContinuousVariable, VariablesList\n",
    "from nemo_bo.opt.objectives import RegressionObjective, ObjectivesList\n",
    "from nemo_bo.acquisition_functions.expected_improvement.expected_improvement import (\n",
    "    ExpectedImprovement,\n",
    ")\n",
    "from nemo_bo.opt.samplers import LatinHyperCubeSampling\n",
    "from nemo_bo.opt.optimisation import Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the variable objects\n",
    "var1 = ContinuousVariable(name=\"variable1\", lower_bound=0.0, upper_bound=100.0)\n",
    "var2 = ContinuousVariable(name=\"variable2\", lower_bound=0.0, upper_bound=100.0)\n",
    "var3 = ContinuousVariable(name=\"variable3\", lower_bound=0.0, upper_bound=100.0)\n",
    "var_list = VariablesList([var1, var2, var3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the objective objects\n",
    "obj1 = RegressionObjective(\n",
    "    name=\"objective1\",\n",
    "    obj_max_bool=True,\n",
    "    lower_bound=0.0,\n",
    "    upper_bound=100.0,\n",
    ")\n",
    "obj2 = RegressionObjective(\n",
    "    name=\"objective2\",\n",
    "    obj_max_bool=False,\n",
    "    lower_bound=0.0,\n",
    "    upper_bound=100.0,\n",
    ")\n",
    "obj_list = ObjectivesList([obj1, obj2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the sampler to use for the optimisation\n",
    "optimisation_sampler = LatinHyperCubeSampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the acquisition function\n",
    "acq_func = ExpectedImprovement(num_candidates=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimisation instance\n",
    "optimisation = Optimisation(var_list, obj_list, acq_func, sampler=optimisation_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to generate initial samples to use as the X array for the training set, `X_training_set`.\n",
    "\n",
    "A different sampler instance is used for generating the training set because a different number of samples will likely be needed compared to the number of samples generated by the acquisition function during an optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples by passing in the VariablesList object\n",
    "training_set_sampler = LatinHyperCubeSampling(num_new_points=2**4)\n",
    "X_training_set = training_set_sampler.generate_samples(var_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the `X_training_set` array, the user would then perform these experiments to obtain the corresponding objective values. These values are demonstrated below as the hypothetical `Y_training_set` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NEMO to suggest Bayesian optimisation candidates\n",
    "\n",
    "The `find_candidates` function can be used to generate Bayesian optimisation candidates by passing in the existing X and Y data. The `model_search_bool` argument is a boolean that defines whether automated model and hyperparameter optimisation is to be performed when fitting the regression models. The `test_ratio` keyword argument is the proportion of inputted X and Y arrays to be split for the validation and test sets where applicable during this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the optimisation using the convenient run function that will run for the specified number of iterations\n",
    "# X and Y arrays represent a hypothetical initial dataset\n",
    "X_candidates, Y_candidates = optimisation.find_candidates(\n",
    "    X_training_set,\n",
    "    Y_training_set,\n",
    "    model_search_bool=model_search_bool,\n",
    "    test_ratio=test_ratio,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7101ca937a6cefa303d920fd1335fe82956cc294edbf5f7bc268a5a56c54bb64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
