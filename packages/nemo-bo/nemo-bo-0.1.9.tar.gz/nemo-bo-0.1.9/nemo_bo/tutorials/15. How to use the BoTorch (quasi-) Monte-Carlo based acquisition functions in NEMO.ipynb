{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the BoTorch (quasi-) Monte-Carlo based acquisition functions in NEMO\n",
    "\n",
    "The (quasi-) Monte-Carlo based acquisition functions in the BoTorch library has demonstrated excellent Bayesian optimisation performance with short wall-times for suggesting new candidates. However, they are not used by default for NEMO's expected improvement based methods because NEMO uses various types of regression models and calculable objectives that do not all have a `posterior` method and do not allow back-propagating gradients from targets back to the inputs. This precludes the use of BoTorch's methods for all of NEMO's native machine learning models except for Gaussian processes (GPs).\n",
    "\n",
    "As the GPs used in NEMO are derived from the BoTorch library, a NEMO optimisation problem that exclusively uses GP models can be exploited to use the `qNoisyExpectedImprovement` and `qExpectedHypervolumeImprovement` acquisition functions from the BoTorch library.\n",
    "\n",
    "This tutorial will demonstrate how to set up an optimisation that can utilise these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the variable, objectives, sampler, acquisition function, and the optimisation classes\n",
    "from nemo_bo.opt.variables import ContinuousVariable, VariablesList\n",
    "from nemo_bo.opt.objectives import RegressionObjective, ObjectivesList\n",
    "from nemo_bo.acquisition_functions.expected_improvement.expected_improvement import (\n",
    "    ExpectedImprovement,\n",
    ")\n",
    "from nemo_bo.opt.samplers import LatinHyperCubeSampling\n",
    "from nemo_bo.opt.optimisation import Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the variable objects\n",
    "var1 = ContinuousVariable(name=\"variable1\", lower_bound=0.0, upper_bound=100.0)\n",
    "var2 = ContinuousVariable(name=\"variable2\", lower_bound=0.0, upper_bound=100.0)\n",
    "var_list = VariablesList([var1, var2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the machine learning model types for objectives to be GPs\n",
    "\n",
    "By specifying the objectives to be GPs, this guarantees that the models will be compatible with the BoTorch methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the objective objects\n",
    "obj1 = RegressionObjective(\n",
    "    name=\"objective1\",\n",
    "    obj_max_bool=True,\n",
    "    lower_bound=0.0,\n",
    "    upper_bound=100.0,\n",
    "    predictor_type=\"gp\",\n",
    ")\n",
    "obj2 = RegressionObjective(\n",
    "    name=\"objective2\",\n",
    "    obj_max_bool=False,\n",
    "    lower_bound=0.0,\n",
    "    upper_bound=100.0,\n",
    "    predictor_type=\"gp\",\n",
    ")\n",
    "obj_list = ObjectivesList([obj1, obj2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the sampler\n",
    "sampler = LatinHyperCubeSampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `force_botorch_ei_methods` keyword argument defines whether to force the expected improvement type methods to use the BoTorch-based ones, as long as the following conditions are met: \n",
    "    1. All objectives are modelled with GPs\n",
    "    2. All constraints, if used, are LinearConstraint objects\n",
    "    3. The sampler chosen is not a PoolBased sampler\n",
    "\n",
    "Please note that above we forced the objectives to be modelled using GPs. Conversely, if we allowed the objectives to take on different machine learning models with `force_botorch_ei_methods` still set to `True`, the BoTorch methods can still be used without forcing for GP models as long as GPs are found to be the best models. If during an optimisation run, the best models varies and is not a GP, then the NEMO-based expected improvement methods are used automatically without raising an Exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the acquisition function\n",
    "acq_func = ExpectedImprovement(num_candidates=4, force_botorch_ei_methods=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimisation instance\n",
    "optimisation = Optimisation(var_list, obj_list, acq_func, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the optimisation using the convenient run function that will run for the specified number of iterations\n",
    "# X and Y arrays represent a hypothetical initial dataset\n",
    "optimisation_data = optimisation.run(X, Y, number_of_iterations=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7101ca937a6cefa303d920fd1335fe82956cc294edbf5f7bc268a5a56c54bb64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
