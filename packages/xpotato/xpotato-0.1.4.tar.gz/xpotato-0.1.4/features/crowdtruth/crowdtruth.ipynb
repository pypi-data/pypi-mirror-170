{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77655d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc -q -O \"ground_truth_cause.csv\" \"https://raw.githubusercontent.com/CrowdTruth/Medical-Relation-Extraction/master/ground_truth_cause.csv\"\n",
    "!wget -nc -q -O \"ground_truth_treat.csv\" \"https://raw.githubusercontent.com/CrowdTruth/Medical-Relation-Extraction/master/ground_truth_treat.csv\"\n",
    "!wget -nc -q -O \"ground_truth_cause.xlsx\" \"https://github.com/CrowdTruth/Medical-Relation-Extraction/blob/master/train_dev_test/ground_truth_cause.xlsx?raw=true\"\n",
    "!wget -nc -q -O \"ground_truth_treat.xlsx\" \"https://github.com/CrowdTruth/Medical-Relation-Extraction/blob/master/train_dev_test/ground_truth_treat.xlsx?raw=true\"\n",
    "!wget -nc -q -O \"food_disease_dataset.csv\" \"https://raw.githubusercontent.com/gjorgjinac/food-disease-dataset/main/food_disease_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1bbb37e-a8bb-4e3d-baac-ae152cc5b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d273b77-56ff-4f75-bc26-608fe2b8bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_treat = pd.read_excel(\"/Users/adamkovacs/data/Medical-Relation-Extraction/train_dev_test/ground_truth_treat.xlsx\", sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d11cbf15-0b83-4be1-8922-3491eb0e0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_cause = pd.read_excel(\"/Users/adamkovacs/data/Medical-Relation-Extraction/train_dev_test/ground_truth_cause.xlsx\", sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e941d522-8686-4f4e-b50b-1f02c5b306da",
   "metadata": {},
   "outputs": [],
   "source": [
    "treat_train = dfs_treat['train']\n",
    "treat_dev = dfs_treat['dev']\n",
    "treat_test = dfs_treat['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99a43262-fd59-428e-96c9-29fe172f0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "treat_train = dfs_cause['train']\n",
    "treat_dev = dfs_cause['dev']\n",
    "treat_test = dfs_cause['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cea5fbc-f56a-4585-b6b0-43337a3ee5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def extract_entities(df):\n",
    "    sen = re.sub(re.escape(df.term1), 'XXX', df.sentence, flags=re.IGNORECASE)\n",
    "    sen = re.sub(re.escape(df.term2), 'YYY', sen, flags=re.IGNORECASE)\n",
    "    return sen.encode('ascii', errors='ignore').decode('utf-8')\n",
    "    #return sen\n",
    "    \n",
    "def extract_labels(df):\n",
    "    expert = df.expert\n",
    "    crowd = df.crowd\n",
    "    label = 0\n",
    "    if expert == 1:\n",
    "        label = 1\n",
    "    elif pd.isnull(expert) and crowd > 0:\n",
    "        label = 1\n",
    "    \n",
    "    return label\n",
    "\n",
    "treat_train['preprocessed_sen'] = treat_train.apply(extract_entities, axis=1)\n",
    "treat_train['label_id'] = treat_train.apply(extract_labels, axis=1)\n",
    "#treat_train['label'] = treat_train.label_id.replace({1: 'TREAT', 0: 'NOT'})\n",
    "treat_train['label'] = treat_train.label_id.replace({1: 'CAUSE', 0: 'NOT'})\n",
    "\n",
    "treat_dev['preprocessed_sen'] = treat_dev.apply(extract_entities, axis=1)\n",
    "treat_dev['label_id'] = treat_dev.apply(extract_labels, axis=1)\n",
    "#treat_dev['label'] = treat_dev.label_id.replace({1: 'TREAT', 0: 'NOT'})\n",
    "treat_dev['label'] = treat_dev.label_id.replace({1: 'CAUSE', 0: 'NOT'})\n",
    "\n",
    "\n",
    "treat_test['preprocessed_sen'] = treat_test.apply(extract_entities, axis=1)\n",
    "treat_test['label_id'] = treat_test.apply(extract_labels, axis=1)\n",
    "#treat_test['label'] = treat_test.label_id.replace({1: 'TREAT', 0: 'NOT'})\n",
    "treat_test['label'] = treat_test.label_id.replace({1: 'CAUSE', 0: 'NOT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b77dc7b-6270-475a-8def-823a1acd9c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xpotato.dataset.dataset import Dataset\n",
    "from xpotato.models.trainer import GraphTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b9224ba-2154-4d94-a7b0-67ad7f7e0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rows = treat_train.iterrows()\n",
    "dev_rows = treat_dev.iterrows()\n",
    "test_rows = treat_test.iterrows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b79b88-9069-41c3-8b8c-61333a1a6c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [(row[1].preprocessed_sen, row[1].label) for row in train_rows]\n",
    "dev_sentences = [(row[1].preprocessed_sen, row[1].label) for row in dev_rows]\n",
    "test_sentences = [(row[1].preprocessed_sen, row[1].label) for row in test_rows]\n",
    "\n",
    "#train_dataset = Dataset(train_sentences, label_vocab={\"TREAT\":1, \"NOT\": 0}, lang='en_bio')\n",
    "train_dataset = Dataset(train_sentences, label_vocab={\"CAUSE\":1, \"NOT\": 0}, lang='en_bio')\n",
    "train_dataset.set_graphs(train_dataset.parse_graphs(graph_format=\"ud\"))\n",
    "\n",
    "#dev_dataset = Dataset(dev_sentences, label_vocab={\"TREAT\":1, \"NOT\": 0}, lang='en_bio')\n",
    "dev_dataset = Dataset(dev_sentences, label_vocab={\"CAUSE\":1, \"NOT\": 0}, lang='en_bio')\n",
    "dev_dataset.set_graphs(dev_dataset.parse_graphs(graph_format=\"ud\"))\n",
    "\n",
    "#test_dataset = Dataset(test_sentences, label_vocab={\"TREAT\":1, \"NOT\": 0}, lang='en_bio')\n",
    "test_dataset = Dataset(test_sentences, label_vocab={\"CAUSE\":1, \"NOT\": 0}, lang='en_bio')\n",
    "test_dataset.set_graphs(test_dataset.parse_graphs(graph_format=\"ud\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb8572f-f32b-4d9f-87ce-241bb89fbf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_dataset.to_dataframe()\n",
    "dev_df = dev_dataset.to_dataframe()\n",
    "test_df = test_dataset.to_dataframe()\n",
    "\n",
    "#train_df.to_pickle(\"crowdtruth_train_dataset_treat_ud.pickle\")\n",
    "#dev_df.to_pickle(\"crowdtruth_dev_dataset_treat_ud.pickle\")\n",
    "#test_df.to_pickle(\"crowdtruth_test_dataset_treat_ud.pickle\")\n",
    "train_df.to_pickle(\"crowdtruth_train_dataset_cause_ud.pickle\")\n",
    "dev_df.to_pickle(\"crowdtruth_dev_dataset_cause_ud.pickle\")\n",
    "test_df.to_pickle(\"crowdtruth_test_dataset_cause_ud.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78020824-8bed-484e-a662-e4b9f1fa2109",
   "metadata": {},
   "source": [
    "## Fourlang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ef0cd23-2051-418c-a5fd-54da1b56042a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-08 13:38:17 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | craft   |\n",
      "| pos       | craft   |\n",
      "| lemma     | craft   |\n",
      "| depparse  | craft   |\n",
      "=======================\n",
      "\n",
      "INFO:stanza:Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | craft   |\n",
      "| pos       | craft   |\n",
      "| lemma     | craft   |\n",
      "| depparse  | craft   |\n",
      "=======================\n",
      "\n",
      "2021-10-08 13:38:17 INFO: Use device: cpu\n",
      "INFO:stanza:Use device: cpu\n",
      "2021-10-08 13:38:17 INFO: Loading: tokenize\n",
      "INFO:stanza:Loading: tokenize\n",
      "2021-10-08 13:38:17 INFO: Loading: pos\n",
      "INFO:stanza:Loading: pos\n",
      "2021-10-08 13:38:17 INFO: Loading: lemma\n",
      "INFO:stanza:Loading: lemma\n",
      "2021-10-08 13:38:17 INFO: Loading: depparse\n",
      "INFO:stanza:Loading: depparse\n",
      "2021-10-08 13:38:18 INFO: Done loading processors!\n",
      "INFO:stanza:Done loading processors!\n",
      "WARNING:root:loading NLP cache from en_nlp_cache...\n",
      "WARNING:root:done!\n",
      "WARNING:root:loading cache from file: cache/UD_FL.json\n",
      "WARNING:root:loaded cache from cache/UD_FL.json with interpretations: ['fl', 'ud']\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 3190/3190 [00:03<00:00, 803.62it/s]\n",
      "2021-10-08 13:38:43 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | craft   |\n",
      "| pos       | craft   |\n",
      "| lemma     | craft   |\n",
      "| depparse  | craft   |\n",
      "=======================\n",
      "\n",
      "INFO:stanza:Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | craft   |\n",
      "| pos       | craft   |\n",
      "| lemma     | craft   |\n",
      "| depparse  | craft   |\n",
      "=======================\n",
      "\n",
      "2021-10-08 13:38:43 INFO: Use device: cpu\n",
      "INFO:stanza:Use device: cpu\n",
      "2021-10-08 13:38:43 INFO: Loading: tokenize\n",
      "INFO:stanza:Loading: tokenize\n",
      "2021-10-08 13:38:43 INFO: Loading: pos\n",
      "INFO:stanza:Loading: pos\n",
      "2021-10-08 13:38:43 INFO: Loading: lemma\n",
      "INFO:stanza:Loading: lemma\n",
      "2021-10-08 13:38:43 INFO: Loading: depparse\n",
      "INFO:stanza:Loading: depparse\n",
      "2021-10-08 13:38:44 INFO: Done loading processors!\n",
      "INFO:stanza:Done loading processors!\n",
      "WARNING:root:loading NLP cache from en_nlp_cache...\n",
      "WARNING:root:done!\n",
      "WARNING:root:loading cache from file: cache/UD_FL.json\n",
      "WARNING:root:loaded cache from cache/UD_FL.json with interpretations: ['fl', 'ud']\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 991.37it/s]\n",
      "2021-10-08 13:39:04 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | craft   |\n",
      "| pos       | craft   |\n",
      "| lemma     | craft   |\n",
      "| depparse  | craft   |\n",
      "=======================\n",
      "\n",
      "INFO:stanza:Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | craft   |\n",
      "| pos       | craft   |\n",
      "| lemma     | craft   |\n",
      "| depparse  | craft   |\n",
      "=======================\n",
      "\n",
      "2021-10-08 13:39:04 INFO: Use device: cpu\n",
      "INFO:stanza:Use device: cpu\n",
      "2021-10-08 13:39:04 INFO: Loading: tokenize\n",
      "INFO:stanza:Loading: tokenize\n",
      "2021-10-08 13:39:04 INFO: Loading: pos\n",
      "INFO:stanza:Loading: pos\n",
      "2021-10-08 13:39:04 INFO: Loading: lemma\n",
      "INFO:stanza:Loading: lemma\n",
      "2021-10-08 13:39:05 INFO: Loading: depparse\n",
      "INFO:stanza:Loading: depparse\n",
      "2021-10-08 13:39:05 INFO: Done loading processors!\n",
      "INFO:stanza:Done loading processors!\n",
      "WARNING:root:loading NLP cache from en_nlp_cache...\n",
      "WARNING:root:done!\n",
      "WARNING:root:loading cache from file: cache/UD_FL.json\n",
      "WARNING:root:loaded cache from cache/UD_FL.json with interpretations: ['fl', 'ud']\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 861.09it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sentences = [(row[1].preprocessed_sen, row[1].label) for row in train_rows]\n",
    "dev_sentences = [(row[1].preprocessed_sen, row[1].label) for row in dev_rows]\n",
    "test_sentences = [(row[1].preprocessed_sen, row[1].label) for row in test_rows]\n",
    "\n",
    "#train_dataset = Dataset(train_sentences, label_vocab={\"TREAT\":1, \"NOT\": 0}, lang='en_bio')\n",
    "train_dataset = Dataset(train_sentences, label_vocab={\"CAUSE\":1, \"NOT\": 0}, lang='en_bio')\n",
    "train_dataset.set_graphs(train_dataset.parse_graphs(graph_format=\"fourlang\"))\n",
    "\n",
    "#dev_dataset = Dataset(dev_sentences, label_vocab={\"TREAT\":1, \"NOT\": 0}, lang='en_bio')\n",
    "dev_dataset = Dataset(dev_sentences, label_vocab={\"CAUSE\":1, \"NOT\": 0}, lang='en_bio')\n",
    "dev_dataset.set_graphs(dev_dataset.parse_graphs(graph_format=\"fourlang\"))\n",
    "\n",
    "#test_dataset = Dataset(test_sentences, label_vocab={\"TREAT\":1, \"NOT\": 0}, lang='en_bio')\n",
    "test_dataset = Dataset(test_sentences, label_vocab={\"CAUSE\":1, \"NOT\": 0}, lang='en_bio')\n",
    "test_dataset.set_graphs(test_dataset.parse_graphs(graph_format=\"fourlang\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a216cdf-25fc-4236-9e8c-435008d9ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xpotato.dataset.utils import save_dataframe\n",
    "\n",
    "train_df = train_dataset.to_dataframe()\n",
    "dev_df = dev_dataset.to_dataframe()\n",
    "test_df = test_dataset.to_dataframe()\n",
    "\n",
    "save_dataframe(train_df, \"crowdtruth_train_dataset_cause_fourlang.tsv\")\n",
    "save_dataframe(dev_df, \"crowdtruth_dev_dataset_cause_fourlang.tsv\")\n",
    "save_dataframe(test_df, \"crowdtruth_test_dataset_cause_fourlang.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7160429f-4a01-4267-ae47-821141110660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<potato.dataset.dataset.Dataset at 0x7fee2da4d040>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa95e3-58dd-4372-a6a2-e0e2f23baf51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
